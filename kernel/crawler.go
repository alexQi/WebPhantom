package kernel

import (
	"context"
	"errors"
	"fmt"
	"noctua/internal/constants"
	"noctua/internal/scheduler"
	"noctua/internal/signer"
	"noctua/kernel/bus"
	"noctua/kernel/crawls/douyin"
	"noctua/kernel/reference"
	"noctua/kernel/session"
	"noctua/pkg/logger"
	"noctua/pkg/utils/encrypt"
	"noctua/pkg/utils/str"
	"noctua/types"
	"sync"
	"sync/atomic"
	"time"
)

const ROUND_MAX = 10
const ROUND_SLEEP = 20
const ROUND_INCR = 48

// CrawlerStatus å®šä¹‰çˆ¬è™«ç®¡ç†å™¨çš„çŠ¶æ€
type CrawlerStatus struct {
	Running        bool                    `json:"running"`        // æ˜¯å¦æ­£åœ¨è¿è¡Œ
	ContextActive  bool                    `json:"contextActive"`  // ä¸Šä¸‹æ–‡æ˜¯å¦æ´»è·ƒ
	CrawlerActive  bool                    `json:"crawlerActive"`  // æ˜¯å¦æœ‰çˆ¬è™«å®ä¾‹
	Channels       map[string]*ChannelInfo `json:"channels"`       // å„é€šé“çŠ¶æ€
	SupportedMedia []string                `json:"supportedMedia"` // æ”¯æŒçš„åª’ä½“å¹³å°
}

// ChannelInfo å®šä¹‰é€šé“çŠ¶æ€
type ChannelInfo struct {
	Length   int `json:"length"`
	Capacity int `json:"capacity"`
}

// CrawlerCreator ç”¨äºåˆ›å»ºçˆ¬è™«å®ä¾‹
type CrawlerCreator func(
	ctx context.Context,
	sessionRegion string,
	sessionManager *session.Manager,
	signClient *signer.SignServerClient,
	eventer *bus.EventBus,
) reference.Crawler

type CrawlerManagerConfig struct {
	SignServEndpoint string
	SchedulerConfig  scheduler.Config
}

// Manager è´Ÿè´£ç®¡ç†çˆ¬è™«ä»»åŠ¡
type CrawlerManager struct {
	ctx                context.Context
	mu                 sync.RWMutex
	wg                 *sync.WaitGroup
	running            atomic.Bool
	crawlerInstance    reference.Crawler
	signServer         *signer.SignServerClient
	eventBus           *bus.EventBus
	sessionManager     *session.Manager
	scheduler          *scheduler.Scheduler
	runtimeChannel     chan types.RuntimeData
	crawlers           map[constants.MediaCode]CrawlerCreator
	mapDataChannel     map[string]chan types.FetchItemChan
	currentCrawlParams *types.CrawlParams // å½“å‰è½®æ¬¡å‚æ•°
}

// NewManager åˆ›å»ºçˆ¬è™«ç®¡ç†å™¨
func NewCrawlerManager(
	ctx context.Context,
	config CrawlerManagerConfig,
	sessionManager *session.Manager,
	eventBus *bus.EventBus,
	scheduler *scheduler.Scheduler,
	runtimeChannel chan types.RuntimeData,
) *CrawlerManager {
	cm := &CrawlerManager{
		ctx:                ctx,
		running:            atomic.Bool{},
		wg:                 &sync.WaitGroup{},
		sessionManager:     sessionManager,
		eventBus:           eventBus,
		runtimeChannel:     runtimeChannel,
		mapDataChannel:     map[string]chan types.FetchItemChan{},
		crawlers:           make(map[constants.MediaCode]CrawlerCreator),
		signServer:         signer.NewSignServerClient(config.SignServEndpoint),
		scheduler:          scheduler,
		currentCrawlParams: &types.CrawlParams{},
	}

	// æ³¨å†ŒæŠ–éŸ³çˆ¬è™«
	cm.Register(constants.MediaCodeDouyin, douyin.NewDouyinCrawler)

	return cm
}

// Register æ³¨å†Œçˆ¬è™«
func (cm *CrawlerManager) Register(media constants.MediaCode, creator CrawlerCreator) {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	cm.crawlers[media] = creator
}

// Create é€šè¿‡å¹³å°åç§°åˆ›å»ºçˆ¬è™«å®ä¾‹
func (cm *CrawlerManager) Create(media constants.MediaCode, sessionRegion string) reference.Crawler {
	cm.mu.RLock()
	defer cm.mu.RUnlock()

	creator, exists := cm.crawlers[media]
	if !exists {
		panic(fmt.Sprintf("Invalid Platform: %s", media))
	}
	return creator(cm.ctx, sessionRegion, cm.sessionManager, cm.signServer, cm.eventBus)
}

// Run å¯åŠ¨çˆ¬è™«ä»»åŠ¡
func (cm *CrawlerManager) Run(crawlParams *types.CrawlParams) error {
	if cm.running.Load() {
		logger.Log.Infof("Crawler %s is already running", crawlParams.MediaCode)
		return nil
	}
	// å‘é€é€šçŸ¥
	cm.runtimeChannel <- types.NewRuntimeData(types.RuntimeEventCodeNotification, types.EventData{
		Title:     "æ•°æ®æ´å¯Ÿ",
		CheckHash: encrypt.Md5(time.Now().Format("2006-01-02 15:04:05")),
		Message:   "ğŸ§ æ™ºè¯†å¼•æ“ä¸Šçº¿ï¼Œå¼€å§‹å¤„ç†ä»»åŠ¡...",
		Optional: types.MessageOptional{
			IsNotify: true,
			IsStore:  true,
			ShowType: "notification",
		},
	})
	// é‡ç½®è°ƒåº¦å™¨çŠ¶æ€
	cm.scheduler.Reset()
	// æ£€æŸ¥æ˜¯å¦å·²åœ¨è¿è¡Œ
	if cm.ctx.Err() != nil {
		cm.ctx = cm.scheduler.Context()
	}
	// ä¿®æ”¹runningä¸ºè¿è¡Œä¸­
	cm.running.Store(true)
	// è®¾ç½®é‡‡é›†QPS
	cm.scheduler.SetQueueQPS(str.GenerateStringKey(crawlParams.MediaCode, "search"), 2)
	cm.scheduler.SetQueueQPS(str.GenerateStringKey(crawlParams.MediaCode, "media"), 6)
	cm.scheduler.SetQueueQPS(str.GenerateStringKey(crawlParams.MediaCode, "user"), 10)
	cm.scheduler.SetQueueQPS(str.GenerateStringKey(crawlParams.MediaCode, "comment"), 6)
	// åˆå§‹åŒ–channel
	cm.mapDataChannel = map[string]chan types.FetchItemChan{
		"media":   make(chan types.FetchItemChan, 1000),
		"comment": make(chan types.FetchItemChan, 1000),
		"user":    make(chan types.FetchItemChan, 1000),
	}
	// è·å–çˆ¬è™«å®ä¾‹
	cm.crawlerInstance = cm.Create(constants.MediaCode(crawlParams.MediaCode), crawlParams.Region)
	// åˆå§‹åŒ–çˆ¬è™«
	cm.crawlerInstance.Initialize(cm.scheduler, cm.runtimeChannel, cm.mapDataChannel)
	// è®¾ç½®åˆå§‹é‡‡é›†å‚æ•°
	cm.currentCrawlParams = crawlParams
	// å¯åŠ¨å¤„ç†æ•°æ®çº¿ç¨‹
	for taskType := range cm.mapDataChannel {
		cm.wg.Add(1)
		go cm.processChannel(cm.mapDataChannel[taskType])
	}
	// å®šä¹‰æ˜¯å¦æœ‰å­ä»»åŠ¡å‚æ•°
	var jobPayloads []interface{}
	// æŠ•é€’å…¥å£ä»»åŠ¡
	switch crawlParams.CrawlType {
	case string(constants.CrawlerTypeSearch):
		for _, keyword := range crawlParams.Keywords {
			jobPayloads = append(jobPayloads, types.SearchParams{
				Keyword:         keyword,
				WithUser:        crawlParams.WithUser,
				WithComment:     crawlParams.WithComment,
				WithCommentUser: crawlParams.WithCommentUser,
				MaxCount:        crawlParams.MaxCount,
				PageSize:        16,
			})
		}
	case string(constants.CrawlerTypeMedia):
		for _, keyword := range crawlParams.Keywords {
			jobPayloads = append(jobPayloads, types.MediaParams{
				Id:              keyword,
				WithUser:        crawlParams.WithUser,
				WithComment:     crawlParams.WithComment,
				WithCommentUser: crawlParams.WithCommentUser,
			})
		}
	case string(constants.CrawlerTypeUser):
		for _, keyword := range crawlParams.Keywords {
			jobPayloads = append(jobPayloads, types.UserParams{
				UserId:           keyword,
				WithAllCreations: crawlParams.WithAllCreations,
				WithComment:      crawlParams.WithComment,
				WithCommentUser:  crawlParams.WithCommentUser,
			})
		}
	default:
		return errors.New("Unsupport CrawlerType")
	}

	roundSignal := make(chan struct{}, ROUND_MAX*2)
	roundSignal <- struct{}{}

	roundWg := &sync.WaitGroup{}
	roundWg.Add(1)
	go func() {
		defer roundWg.Done()
		// å¤„ç†æ•°æ®è½®æ¬¡
		currentRound := 0
		for {
			select {
			case <-roundSignal:
				// è¶…å‡ºæ¬¡æ•°ï¼Œè·³å‡ºå¾ªç¯
				if currentRound >= ROUND_MAX {
					return
				}
				logger.Log.Infof("Start crawl task round check %dï¼ŒMediaCode %s", currentRound, crawlParams.MediaCode)
				// æäº¤ä»»åŠ¡
				for _, payload := range jobPayloads {
					err := cm.crawlerInstance.SubmitJob(
						crawlParams.CrawlType, payload, scheduler.TaskOptions{},
					)
					if err != nil {
						logger.Log.Error(err.Error())
					}
				}
				// ç­‰å¾…æœ¬è½®ä»»åŠ¡å®Œæˆ
				cm.scheduler.WaitUntilEmpty()
				// å¢åŠ è½®æ¬¡è®¡æ•°
				currentRound++
				// å¦‚æœè¿˜æœ‰ä¸‹ä¸€è½®ï¼Œæš‚åœä¸€æ®µæ—¶é—´ï¼ˆå¯é…ç½®ï¼‰
				if currentRound < ROUND_MAX {
					select {
					case <-time.After(time.Duration(ROUND_SLEEP) * time.Second):
					case <-cm.ctx.Done():
						return
					}
				}
				// æ·»åŠ ä¿¡å·
				roundSignal <- struct{}{}
			case <-cm.ctx.Done():
				return
			}
		}
	}()

	roundWg.Wait()

	if cm.ctx.Err() == nil {
		cm.eventBus.Publish(types.CrawlEndEvent{
			Code:      types.CrawlEndCodeRoundMaxed,
			ReceiveAt: time.Now(),
		})
	}

	cm.cleanup()

	return nil
}

// processChannel é€šç”¨é€šé“å¤„ç†å‡½æ•°
func (cm *CrawlerManager) processChannel(ch chan types.FetchItemChan) {
	defer cm.wg.Done()
	for {
		select {
		case item, ok := <-ch:
			if !ok {
				return
			}
			if item.Data == nil {
				continue
			}
			cm.mu.RLock()
			params := cm.currentCrawlParams
			cm.mu.RUnlock()
			if err := cm.crawlerInstance.HandleChannel(item, params); err != nil {
				logger.Log.Errorf("TaskID=%s: SubmitSubTasks error: %v", item.TaskId, err)
			}
		case <-cm.ctx.Done():
			return
		}
	}
}

// Status è¿”å›çˆ¬è™«ç®¡ç†å™¨çš„å½“å‰çŠ¶æ€
func (cm *CrawlerManager) Status() *CrawlerStatus {
	cm.mu.RLock()
	defer cm.mu.RUnlock()

	// é€šé“çŠ¶æ€
	channels := make(map[string]*ChannelInfo)
	for key, ch := range cm.mapDataChannel {
		channels[key] = &ChannelInfo{
			Length:   len(ch),
			Capacity: cap(ch),
		}
	}

	// æ”¯æŒçš„åª’ä½“å¹³å°
	supportedMedia := make([]string, 0, len(cm.crawlers))
	for media := range cm.crawlers {
		supportedMedia = append(supportedMedia, media.String())
	}

	return &CrawlerStatus{
		Running:        cm.running.Load(),
		ContextActive:  cm.ctx.Err() == nil,
		CrawlerActive:  cm.crawlerInstance != nil,
		Channels:       channels,
		SupportedMedia: supportedMedia,
	}
}

// Stop åœæ­¢çˆ¬è™«ä»»åŠ¡
func (cm *CrawlerManager) Stop() {
	if !cm.running.Load() {
		return
	}
	cm.eventBus.Publish(types.CrawlEndEvent{
		Code:      types.CrawlEndCodeForcedStop,
		ReceiveAt: time.Now(),
	})
}

// cleanup æ¸…ç†èµ„æº
func (cm *CrawlerManager) cleanup() {
	// å…³é—­æ‰€æœ‰chan
	for _, channel := range cm.mapDataChannel {
		close(channel)
	}
	cm.mapDataChannel = make(map[string]chan types.FetchItemChan)
	// ç­‰å¾…é‡‡é›†ç¨‹åºprocessç»“æŸ
	cm.wg.Wait()
	// åˆ é™¤å½“å‰çˆ¬è™«å®ä¾‹
	cm.crawlerInstance = nil
	cm.running.Store(false)

	// æ¸…é™¤é‡‡é›†å‚æ•°
	cm.mu.Lock()
	cm.currentCrawlParams = nil
	cm.mu.Unlock()
}
